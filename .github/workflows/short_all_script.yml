name: Run short all script (safe local-llm support)

on:
  workflow_dispatch:
    inputs:
      install_local_llm:
        description: 'true => install heavy LLM deps (llama-cpp, torch/transformers).'
        required: false
        default: 'false'
      download_model:
        description: 'true => attempt to download model file from HF anonymously (if public).'
        required: false
        default: 'true'
      use_model_artifact:
        description: 'true => upload downloaded model as artifact for reuse.'
        required: false
        default: 'true'
      require_local:
        description: 'true => enforce local-only (script will raise if local model missing).'
        required: false
        default: 'false'
      model_repo:
        description: 'Hugging Face repo id (used to build anonymous URL).'
        required: false
        default: 'mistralai/Mistral-small-3.1'
      model_filename:
        description: 'Filename in the repo to download.'
        required: false
        default: 'mistral-small-3.1.gguf'

jobs:
  run-script:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    env:
      R_HOURS: "6"
      PYTHON_VERSION: "3.11"
      NODE_VERSION: "18"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Show disk & prepare /mnt
        run: |
          echo "=== start disk ==="
          df -h
          sudo mkdir -p /mnt/models /mnt/ci_tmp /mnt/pip_cache /mnt/npm_cache /mnt/playwright_browsers
          sudo chown $USER:$USER /mnt/models /mnt/ci_tmp /mnt/pip_cache /mnt/npm_cache /mnt/playwright_browsers
          echo "TMPDIR=/mnt/ci_tmp" >> $GITHUB_ENV
          echo "PIP_CACHE_DIR=/mnt/pip_cache" >> $GITHUB_ENV
          echo "NPM_CONFIG_CACHE=/mnt/npm_cache" >> $GITHUB_ENV
          echo "PLAYWRIGHT_BROWSERS_PATH=/mnt/playwright_browsers" >> $GITHUB_ENV
          df -h

      - name: Install light Python deps (always)
        run: |
          python -m pip install --upgrade pip setuptools wheel
          if [ -f requirements.txt ]; then pip install --prefer-binary -r requirements.txt ; fi
          pip install --prefer-binary --no-deps requests cloudscraper imageio-ffmpeg playwright || true
          python -m playwright install --with-deps || true

      - name: Install heavy local-LLM deps (optional)
        if: ${{ github.event.inputs.install_local_llm == 'true' }}
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake libgomp1 libopenblas-dev || true
          python -m pip install --upgrade pip setuptools wheel
          # Install llama-cpp-python (preferred for gguf), huggingface-hub and transformers
          python -m pip install --prefer-binary "huggingface-hub>=0.18.4" "transformers[torch]>=4.35.2" accelerate safetensors sentencepiece || true
          python -m pip install --prefer-binary --extra-index-url https://download.pytorch.org/whl/cpu torch || true
          # llama-cpp-python can often be installed as a wheel; if not, pip will attempt to build it (needs cmake)
          python -m pip install --prefer-binary "llama-cpp-python" || true

      - name: Download model (anonymous if public)
        id: download-model
        if: ${{ github.event.inputs.download_model == 'true' }}
        run: |
          set -e
          MODEL_DIR=/mnt/models
          mkdir -p "$MODEL_DIR"
          MODEL_FILE="${{ github.event.inputs.model_filename }}"
          TARGET="$MODEL_DIR/$MODEL_FILE"
          if [ -f "$TARGET" ]; then
            echo "downloaded=true" >> $GITHUB_OUTPUT
            echo "path=$TARGET" >> $GITHUB_OUTPUT
            exit 0
          fi
          REPO="${{ github.event.inputs.model_repo }}"
          URL="https://huggingface.co/${REPO}/resolve/main/${MODEL_FILE}"
          echo "Attempting anonymous download from $URL"
          # Use wget with resume
          wget -c "$URL" -O "$TARGET" || true
          if [ -f "$TARGET" ]; then
            echo "downloaded=true" >> $GITHUB_OUTPUT
            echo "path=$TARGET" >> $GITHUB_OUTPUT
          else
            echo "downloaded=false" >> $GITHUB_OUTPUT
          fi

      - name: Upload model artifact (optional)
        if: ${{ github.event.inputs.use_model_artifact == 'true' && steps.download-model.outputs.downloaded == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: llm-model-${{ github.run_id }}
          path: ${{ steps.download-model.outputs.path }}

      - name: Show model file (verify)
        run: |
          if [ -f "/mnt/models/${{ github.event.inputs.model_filename }}" ]; then
            echo "Model present:" && ls -lh "/mnt/models/${{ github.event.inputs.model_filename }}"
            file "/mnt/models/${{ github.event.inputs.model_filename }}" || true
            sha256sum "/mnt/models/${{ github.event.inputs.model_filename }}" | cut -c1-64 || true
          else
            echo "Model not present at /mnt/models/${{ github.event.inputs.model_filename }}"
          fi
        continue-on-error: true

      - name: Run your scripts (llm + pipeline)
        working-directory: Script_Drive1
        env:
          REQUIRE_LOCAL: ${{ github.event.inputs.require_local }}
          ALLOW_FALLBACK: ${{ github.event.inputs.require_local == 'true' && 'false' || 'true' }}
          LOCAL_MODEL_PATH: "/mnt/models/${{ github.event.inputs.model_filename }}"
          LOCAL_DEVICE: "cpu"
          LOCAL_BACKEND: "llama_cpp"
          HF_TOKEN: ${{ secrets.HF_TOKEN || '' }}   # optional, empty for anonymous
          R_HOURS: "6"
        run: |
          echo "REQUIRE_LOCAL=$REQUIRE_LOCAL"
          echo "ALLOW_FALLBACK=$ALLOW_FALLBACK"
          echo "LOCAL_MODEL_PATH=$LOCAL_MODEL_PATH"
          # Export to environment variables the Python code can read, or pass into StoryPipeline (it reads env/defaults)
          export LOCAL_MODEL="$LOCAL_MODEL_PATH"
          export LOCAL_BACKEND="$LOCAL_BACKEND"
          export LOCAL_DEVICE="$LOCAL_DEVICE"
          export REQUIRE_LOCAL="$REQUIRE_LOCAL"
          export ALLOW_FALLBACK="$ALLOW_FALLBACK"
          # Run the main script - llm.py reads env / args; defaults will fall back to remote if allowed
          python llm.py

      - name: Upload debug artifacts (if present)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: llm-debug-artifacts
          path: |
            Script_Drive1/debug_proxies/**
            Script_Drive1/generated_complete_script/**
            Script_Drive1/image_response/**
            Script_Drive1/*.log
            Script_Drive1/youtube_response/**
            Script_Drive1/narration_response/**

      - name: Show disk (end)
        if: always()
        run: |
          echo "=== end disk ==="
          df -h
          du -sh /mnt/* 2>/dev/null || true
