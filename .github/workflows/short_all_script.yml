name: Run short all script

on:
  workflow_dispatch:
    inputs:
      hf_key:
        description: 'Hugging Face token (runtime input). This will be passed to the job as HF_TOKEN.'
        required: true
        type: string

jobs:
  run-script:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    env:
      HF_TOKEN: ${{ github.event.inputs.hf_key }}
      MODEL_DEST_PATH: ${{ github.workspace }}/models/mistral-small-3.1.gguf
      REPO_ID: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
      USE_AUTH: "true"
      SELECT_STRATEGY: "auto"
      VERBOSE: "true"
      # CI cache dirs (overridden below to workspace so we can delete them)
      TRANSFORMERS_CACHE: ${{ github.workspace }}/hf_cache
      HF_HOME: ${{ github.workspace }}/hf_cache
      PIP_CACHE_DIR: ${{ github.workspace }}/pip_cache

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Show disk (before)
        run: df -h

      - name: Prepare removable cache dirs
        run: |
          mkdir -p $GITHUB_WORKSPACE/hf_cache
          mkdir -p $GITHUB_WORKSPACE/pip_cache
          mkdir -p $GITHUB_WORKSPACE/models

      - name: Install CPU-only torch wheel
        run: |
          python -m pip install --upgrade pip
          python -m pip install --no-cache-dir --index-url https://download.pytorch.org/whl/cpu \
            "torch==2.2.2+cpu" --find-links https://download.pytorch.org/whl/cpu/torch_stable.html

      - name: Install CI dependencies (tiny set)
        run: |
          # Use a CI-only requirements file (see repo: requirements-ci.txt)
          python -m pip install --no-cache-dir --prefer-binary -r requirements-ci.txt
          python -m pip cache purge || true

      - name: Show disk (after install)
        run: df -h

      - name: Show environment (debug)
        run: |
          echo "HF_TOKEN set? -> ${HF_TOKEN:+YES}"
          echo "MODEL_DEST_PATH -> $MODEL_DEST_PATH"
          echo "REPO_ID -> $REPO_ID"
          echo "SELECT_STRATEGY -> $SELECT_STRATEGY"
          python -c "import psutil,sys; print('Total RAM GB:', psutil.virtual_memory().total/1024**3 if 'psutil' in sys.modules else 'psutil missing')"

      - name: Run download & test script
        run: |
          cd Script_Drive1
          python3 test_local_llm.py
        env:
          HF_TOKEN: ${{ github.event.inputs.hf_key }}
          MODEL_DEST_PATH: ${{ github.workspace }}/models/Llama-2-7B-Chat-GGUF.gguf
          REPO_ID: TheBloke/Mistral-7B-Instruct-v0.2-GGUF

      - name: Show disk (after test)
        run: df -h

      - name: Run your script (llm)
        env:
          R_HOURS: "6"
        run: |
          cd Script_Drive1
          python local_llm.py || true
          # aggressively remove any models / tokens / HF cache created during run
          rm -rf $GITHUB_WORKSPACE/models/* || true
          rm -rf $GITHUB_WORKSPACE/hf_cache || true
          python -m pip cache purge || true

      - name: Show disk (after run)
        run: df -h

      - name: Upload debug artifacts (if present)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: llm-debug-artifacts
          path: |
            Script_Drive1/generated_complete_script/**
            Script_Drive1/image_response/**
            Script_Drive1/youtube_response/**
            Script_Drive1/narration_response/**

      - name: Final cleanup (workspace & temp)
        if: always()
        run: |
          echo "Cleaning workspace..."
          # remove workspace contents but keep the workspace dir itself
          rm -rf "$GITHUB_WORKSPACE"/* || true
          rm -rf /tmp/* || true
          rm -rf /home/runner/work/_temp/* || true
          rm -rf /home/runner/work/_tool/* || true
          docker system prune -af --volumes || true

      - name: Show disk (final)
        run: df -h
